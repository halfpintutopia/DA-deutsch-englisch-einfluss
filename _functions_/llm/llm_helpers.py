#!/usr/bin/env python
# coding: utf-8

import ollama
import pandas as pd
from typing import Optional
from tqdm import tqdm
import time
import logging
import os
import json


def ask_ollama(
    prompt: str,
    model: str = "mistral",
    system: Optional[str] = None,
    retries: int = 3,
    delay: float = 2.0
) -> str:
    """
    Sends a prompt to an Ollama language model and returns the generated response.

    This function communicates with the Ollama chat API, optionally including a system
    message to guide the model's behaviour. It supports retry logic for improved robustness
    in case of transient errors during the API call.

    Parameters:
        prompt (str): The user message to send to the language model.
        model (str, optional): The name of the Ollama model to use (default is "mistral").
        system (str, optional): An optional system message to influence model behaviour.
        retries (int, optional): Number of retry attempts if an error occurs (default is 3).
        delay (float, optional): Delay in seconds between retry attempts (default is 2.0).

    Returns:
        str: The content of the model's response if successful, or "error" if all retries fail.
    """
    messages = []

    if system:
        messages.append({"role": "system", "content": system})
    messages.append({"role": "user", "content": prompt})

    for attempt in range(retries):
        try:
            response = ollama.chat(model=model, messages=messages)
            return response["message"]["content"].strip()
        except Exception as e:
            print(f"[Ollama error - attempt {attempt+1}] {e}")
            time.sleep(delay)
        return "error"

    response = ollama.chat(model=model, messages=[
        {"role": "system", "content": system} if system else {},
        {"role": "user", "content": prompt}
    ])
    return response["message"]["content"]


def classify_tone(text: str) -> str:
    """
    Determines whether the tone of the given text is formal or informal.

    This function uses a language model to analyze the tone of the input text
    and classify it as either "formal" or "informal" based on its style and language.

    Parameters:
        text (str): The input text to be analysed.

    Returns:
        str: A lowercase string, either "formal" or "informal", representing the detected tone.
    """
    prompt = (
        "Is the following article written in a formal or informal tone? "
        "Respond with only one word: 'formal' or 'informal'.\n\n"
        f"Text:\n{text}"
    )
    return ask_ollama(prompt=prompt).strip().lower()


def classify_topic(text: str) -> str:
    """
    Classifies the topic of a given text into one of several predefined categories.

    Categories include: 'business', 'technology', 'lifestyle', 'politics', or 'culture'.
    The classification is performed using a language model.

    Parameters:
        text (str): The input text to classify.

    Returns:
        str: One of the category labels in lowercase.
    """
    prompt = (
        "Classify the following article as one of the following categories: "
        "'business', 'technology', 'lifestyle', 'politics', 'culture'. "
        "Respond with only one word.\n\n"
        f"Text:\n{text}"
    )

    return ask_ollama(prompt=prompt).strip().lower()


def summarise_article(text: str) -> str:
    """
    Generates a concise 2â€“3 sentence summary of a German-language article.

    Parameters:
        text (str): The full text of the German article.

    Returns:
        str: A brief summary generated by the language model.
    """
    prompt = (
        "Summarise the German article in 2-3 sentences:\n\n"
        f"{text}"
    )

    return ask_ollama(prompt=prompt)


def explain_loanwords_usage(text: str) -> str:
    """
    Analyses the presence of English loanwords in a German article and explains their purpose.

    The explanation may include insights into the article's context or target audience
    based on the use of these English terms.

    Parameters:
        text (str): The full text of the German article.

    Returns:
        str: A textual explanation of the loanword usage.
    """
    prompt = (
        "Why does this German article use English words? "
        "What might this say about the context or target audience?\n\n"
        f"{text}"
    )

    return ask_ollama(prompt=prompt)


def detect_marketing_loanwords(text: str) -> str:
    """
    Identifies English loanwords in a German article that are used in a marketing or advertising context.

    Parameters:
        text (str): The text of the article.

    Returns:
        str: A list of English words used in promotional or branding contexts.
    """
    prompt = (
        "Here is an article in German:\n\n"
        f"{text}"
        "Which of these English loanwords are used in a marketing or advertising context? "
        "Return a list."
    )

    return ask_ollama(prompt=prompt)


def detect_country_influence(text: str) -> str:
    """
    Detects which country's culture is most reflected in a German article.

    Analyses the article to identify cultural influences from a predefined list of influential countries.
    Returns results in JSON format including both the country/countries and reasoning.

    Parameters:
        text (str): The text of the German article.

    Returns:
        str: JSON string with two fields:
            - "countries": a list of influenced countries
            - "reason": a brief explanation of the influence
    """
    prompt = (
        "Does this German article show cultural influence from any of the following influential countries: "
        "USA, China, Russia, India, France, Germany, UK, Japan, Saudi Arabia, Italy, Canada, Israel, Australia, Spain, South Korea, Turkey, Switzerland, Iran. "
        "If there is influence from more than one, respond with all relevant countries. "
        "If the influence comes from a country not on this list, name the specific country or countries explicitly. "
        "Provide your response in JSON format with two fields:\n"
        "1. \"countries\": a list of influenced countries\n"
        "2. \"reason\": a short explanation of the cultural influence observed\n\n"
        f"{text}"
    )

    return ask_ollama(prompt=prompt)


def detect_unwanted_loanwords(text: str, loanwords: list[str]) -> list[str]:
    """
    Filters out English words in a German article that are generic UI terms, boilerplate, brand names,
    or social media platforms, and should not be treated as meaningful loanwords.

    Parameters:
        text (str): The full article text.
        loanwords (list[str]): A list of detected English loanwords.

    Returns:
        list[str]: A list of words that are considered irrelevant for loanword analysis.
    """
    prompt = (
        "Here is a German article and a list of English loanwords that appear in it: \n\n"
        f"Text: {text}\n\n"
        f"Loanwords: {', '.join(loanwords)}\n\n"
        "Which of these words are likely to be generic UI or boilerplate terms "
        "such as 'footer', 'ticker', 'tracking', or brand names and social media platforms "
        "that should not be considered true loanwords? Return a list of these irrelevant words."
        f"{text}"
    )

    response = ask_ollama(prompt=prompt)
    return [w.strip() for w in response.split(",") if w.strip()]


def batch_clean_loanwords(
    df: pd.DataFrame,
    index_column: str = "article_id",
    limit: int = None,
    checkpoint_path: str = "loanwords_progress.csv",
    log_path: str = "loanwords_processing.log"
) -> pd.DataFrame:
    """
    Processes a batch of articles to refine English loanwords by removing irrelevant ones.

    Maintains a checkpoint CSV to track progress across runs and log errors. The function 
    removes UI/boilerplate terms from loanword lists and stores cleaned data.

    Parameters:
        df (pd.DataFrame): DataFrame containing article data.
        index_column (str): Column used as a unique identifier for articles.
        limit (int, optional): Optional limit on number of rows to process.
        checkpoint_path (str): Path to save progress for resumability.
        log_path (str): Path to save processing logs.

    Returns:
        pd.DataFrame: DataFrame including original article IDs with updated loanword lists.
    """
    logging.basicConfig(filename=log_path, level=logging.INFO,
                        format="%(asctime)s - %(message)s")

    processed_ids = set()
    if os.path.exists(checkpoint_path):
        logging.info(f"Loading checkpoint from {checkpoint_path}")
        processed_df = pd.read_csv(checkpoint_path)
        if index_column in processed_df.columns:
            processed_ids = set(processed_df[index_column])
        else:
            processed_df = pd.DataFrame()
    else:
        processed_df = pd.DataFrame()

    to_process = df[~df[index_column].isin(processed_ids)]

    if limit:
        to_process = to_process.head(limit)

    new_results = []

    for _, row in tqdm(to_process.iterrows(), total=to_process.shape[0], desc="Cleaning loanwords"):
        idx = row[index_column]
        try:
            excluded = detect_unwanted_loanwords(row["text"], row["loanwords"])
            refined = [w for w in row["loanwords"] if w not in excluded]

            result_row = {
                index_column: idx,
                "excluded_loanwords": excluded,
                "refined_loanwords": refined
            }
            new_results.append(result_row)
            logging.info(f"Processed row: {idx}")
        except Exception as e:
            logging.error(f"Error processing row {idx}: {e}")

    new_df = pd.DataFrame(new_results)
    combined_df = pd.concat([processed_df, new_df], ignore_index=True)

    combined_df["excluded_loanwords"] = combined_df["excluded_loanwords"].apply(
        json.dumps)
    combined_df["refined_loanwords"] = combined_df["refined_loanwords"].apply(
        json.dumps)

    combined_df.to_csv(checkpoint_path, index=False)
    return combined_df


def enrich_article_and_create_dataframe(
    df: pd.DataFrame,
) -> pd.DataFrame:
    """
    Adds enrichment columns to a DataFrame of articles using various Ollama-based NLP tasks.

    For each article, computes tone, topic, summary, loanword usage explanation, 
    marketing loanwords, and country influence.

    Parameters:
        df (pd.DataFrame): DataFrame containing a column named "text" with article content.

    Returns:
        pd.DataFrame: New DataFrame with enrichment results for each article.
    """
    df_copy = df.copy()

    results = {
        "tone": [],
        "topic": [],
        "summary": [],
        "loanwords_usage": [],
        "marketing_loanwords": [],
        "country_influence": []
    }

    for txt in tqdm(df_copy["text"], desc="Enriching with Ollama"):
        results["tone"].append(classify_tone(text=txt))
        results["topic"].append(classify_topic(text=txt))
        results["summary"].append(summarise_article(text=txt))
        results["loanwords_usage"].append(explain_loanwords_usage(text=txt))
        results["marketing_loanwords"].append(
            detect_marketing_loanwords(text=txt))
        results["country_influence"].append(detect_country_influence(text=txt))

    return pd.DataFrame(results)


def add_id_to_df(
    df: pd.DataFrame,
    column_name: str,
    # suffix: str,
    insert_index: bool = True
) -> pd.DataFrame:
    """
    Adds a unique identifier column to a DataFrame based on row index.

    Optionally resets the index before assigning IDs. Useful for downstream tracking or merging.

    Parameters:
        df (pd.DataFrame): The original DataFrame.
        column_name (str): Name of the new identifier column.
        insert_index (bool): Whether to drop and reassign index before creating the ID column.

    Returns:
        pd.DataFrame: Modified DataFrame with the new identifier column.
    """
    df_copy = df.copy()

    df_copy = df_copy.reset_index(drop=insert_index)

    df_copy[f"{column_name}"] = df_copy.index


def load_cleaned_progress(
    csv_path: str = "loanwords_progress.csv"
) -> pd.DataFrame:
    """
    Loads a previously saved loanword refinement CSV and parses its JSON-formatted columns.

    Parameters:
        csv_path (str): Path to the saved CSV file.

    Returns:
        pd.DataFrame: DataFrame with properly loaded and parsed JSON columns.
    """
    df = pd.read_csv(csv_path)
    df["excluded_loanwords"] = df["excluded_loanwords"].apply(json.loads)
    df["refined_loanwords"] = df["refined_loanwords"].apply(json.loads)
    return df
